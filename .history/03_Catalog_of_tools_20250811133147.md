Comprehensive Catalog of All Recommendations
> Role: Canonical Guardrails & Tools Catalog. Sources merged from `02_reserach_backed_tools.md` and blueprint `00_complete_...w0-w9_reserch.md`. Risk register: `01_ai_dev_pitfalls.md`. Live status: `00b_Master_Plan_remediation_log.md`.
Document ID: REC_CATALOG_V1
Status: SYNTHESIZED & AWAITING REVIEW
Purpose: A complete, organized inventory of every strategy, framework, guardrail, tool, and process recommended across all provided documents. This catalog forms the basis for our Phase -1 research and analysis.

I. Core Philosophy & Development Methodology

Composer-First Strategy: The foundational idea of leveraging existing, production-proven components, platforms (like LangGraph), and templates instead of building everything from scratch.

Test-Driven Development (TDD): A methodology where tests are written before the application code. The code is then written to pass these tests. It acts as a machine-enforced contract.

Behavior-Driven Development (BDD): A variation of TDD that uses natural language syntax (Gherkin: Given/When/Then) to define tests, making them readable by non-technical founders.

Specification-First Pipeline: The principle that business logic must first be defined in an unambiguous, often machine-readable format (the "spec") before any code is generated.

Executable Documentation: The concept of writing specifications (like BDD scenarios) that are not just documents but are also executable tests.

Graduated Autonomy: The approach of starting with minimal human oversight and gradually phasing it out as the automated system proves its reliability over time, tracked by a "Trust Score."

Start Tiny: The recommendation to prove the entire process on a small "Hello World" project before attempting to build the full application.

II. AI Workforce & Collaboration Architecture

Adversarial Trinity: A multi-agent system with distinct roles to create checks and balances:

Builder AI: Generates the code (Recommended: Claude 3.5 Sonnet).

Auditor AI: Validates the code against business rules and specifications (Recommended: GPT-4o).

Saboteur AI: Actively tries to find edge cases and break the system (Recommended: Gemini).

Human-AI Collaboration Analytics: Instrumenting and tracking metrics like AI suggestion rejection rates or human approval latency to spot "rubber-stamping" or ineffective collaboration patterns.

"Explain Like I'm Five" Gate: A mandatory process where the AI must explain its design decisions in simple, plain English. The change is rejected if the explanation is not understandable.

III. Business Logic & Context Injection

Business Context Package ("Context Saturation"): Creating a single, comprehensive repository of all business-related information (mission, PRDs, user stories, past failure post-mortems, etc.) that is automatically fed to the AI agents with every prompt.

"Time Machine" Validation: Creating a permanent regression test suite based on the specific details of all five previous project failures. Every new build must pass these historical failure tests.

Executable Business Rules: Defining business logic directly as code, for example, using Python decorators (@business_rule) that the AI's generated code must satisfy.

Economic Simulation: Running thousands of simulated user sessions against the application to check for economic exploits or whether high-level business goals (like revenue) are being met.

Semantic Diff: A validation approach that tests not just if the code works, but if it has the same meaning and achieves the same user outcomes as intended.

IV. The Multi-Layer Testing & Validation Stack

This is the suite of advanced testing techniques that sit on top of a standard TDD/BDD framework.

Property-Based Testing: Automatically generates hundreds of randomized test cases from a single rule ("property"), exposing edge cases humans wouldn't think to test.

Tools: Hypothesis (Python), fast-check (JS), RapidCheck (C++).

Mutation Testing: Automatically introduces small bugs (mutations) into the code to verify that the existing tests are strong enough to fail and catch them.

Tools: PITEST (Java), Stryker (JS/NET), MutPy (Python).

Metamorphic Testing: Tests the relationships between inputs and outputs. (e.g., "If I search for 'A', then search for 'B', then search for 'A' again, the results for 'A' should be the same both times").

Frameworks: MORTAR.

Differential Testing (Fuzzing): Runs the same test against two different implementations of a feature to see if they produce different results, flagging potential bugs.

Formal & Bounded Verification: Uses mathematical proofs to verify that critical code paths (like payment processing) are logically correct and free from certain classes of bugs.

Tools: Dafny, TLA+, Alloy.

LLM-Specific Evaluation: A layer of testing focused specifically on the output of Large Language Models.

Purpose: To score responses on relevance, hallucination, bias, and toxicity.

Tools: DeepEval, Giskard, LangSmith Evaluators.

Tracing & Explainability: Tools that provide insight into the AI's decision-making process, helping to debug why a test failed.

Tools: TruLens.

Time-Series Validation: Tracking key metrics (code complexity, test coverage, error rates) over time and automatically triggering rollbacks if a negative trend is detected.

V. The Guardrail Stack (Code Quality, Security & Compliance)

This is the full, synthesized list of the 20 guardrails and the specific tools recommended to implement them.

#1. Style & Quality Gates: Enforces consistent coding style.

Tools: Ruff, GitHub Super-Linter, Flake8 (+ plugins), Black, isort, docformatter, autoflake, Unimport.

#2. Security Linters & SAST: Static analysis for security vulnerabilities.

Tools: SonarQube, Bandit, Semgrep, CodeQL, Qodana, CodeSonar, Snyk Code.

#3. Dependency Upkeep: Automatically updates dependencies to patch vulnerabilities.

Tools: Renovate, Dependabot.

#4. License & Compliance: Scans for non-compliant open-source licenses.

Tools: FOSSA, GitHub Native SBOM, Black Duck, Licensee, CycloneDX/syft for SBOM.

#5. Docs-as-Code: Ensures documentation is versioned and updated with code.

Tools: MkDocs, DocFX, Sphinx, docstr-coverage, OpenAPI/Swagger (for API docs).

#6. Test Generation & Flake Hunting: Uses AI to generate tests and identifies unreliable ("flaky") ones.

Tools: Diffblue Cover, CodiumAI, Codecov.

#7. Build & CI Orchestration: Ensures fast and reproducible builds.

Tools: Gradle, Bazel, Reusable GitHub Workflows.

#8. Code-Review Automation: Uses AI for first-pass code reviews.

Tools: CodeRabbit, Danger.js, GitHub Copilot Reviewer, Graphite, Greptile, Zencoder, Sourcery, Augment Code.

#9. Pre-Commit Hook Framework: Runs checks locally before code is ever committed.

Tools: pre-commit (run Ruff, Black, isort, docformatter, autoflake locally before commit).

#10. AI-Shortcut / Provenance Detection: Scans for AI-generated code that may be a copy of licensed code.

Tools: Black Duck Snippet API.

#11. Observability & Dev Analytics: Provides metrics on the development process itself (DORA metrics).

Tools: Jellyfish.

#12. Release & Deployment Automation: Automates the release process.

Tools: Argo CD, semantic-release.

#13. Secrets & Credential Hygiene: Prevents API keys and other secrets from being committed.

Tools: GitGuardian.

#14. Supply-Chain Signing & Attestation: Cryptographically signs artifacts to prevent tampering.

Tools: Sigstore (cosign), SLSA GitHub Generator, Notary.

#15. IaC & Cloud-Posture Scanning: Scans Infrastructure as Code (e.g., Terraform) for misconfigurations.

Tools: Checkov, TFSec, TFLint, OPA/Gatekeeper, Spacelift, Terragrunt.

#16. Fuzz & Dynamic Security Testing (DAST): Actively probes a running application for vulnerabilities.

Tools: OWASP ZAP, Jazzer, Burp Suite Enterprise, Checkmarx DAST.

#17. Full-Stack Static Analysis: Traces data flow across the entire application to find complex bugs.

Tools: Semgrep.

#18. Runtime Observability & Performance: Collects metrics, logs, and traces from the live application.

Tools: OpenTelemetry, Grafana LGTM stack, Prometheus, Sentry, ELK/EFK stack, Datadog, New Relic, Pyroscope/Parca.

#19. LLM/ML Validation & Drift Detection: Enforces output structure for LLM calls and detects model performance degradation.

Tools: Guardrails AI, NeMo-Guardrails, Evidently AI.

#20. Chaos & Resilience Engineering: Intentionally injects failures to test system resilience.

Tools: LitmusChaos, Gremlin, Chaos Monkey.

Appendix A: Merge Note (from 02_reserach_backed_tools.md)

The separate tools list in `02_reserach_backed_tools.md` has been merged into this catalog. Any tools unique to that file have been folded into the relevant sections above (e.g., CodeSonar, Qodana, Licensee, TFLint, Graphite, Greptile). Treat this file as the single source of truth going forward.

Technical Debt & Clone Gates: A specific guardrail to prevent code duplication and complexity.

Tools: jscpd, radon.

VI. Process, Operations & Human Oversight

Environment Consistency Management: A strict protocol for fingerprinting, replicating, and verifying the execution environment to ensure validation is accurate.

Micro-Task Specification: A detailed template for defining small, well-scoped tasks with clear objectives, budgets, and success criteria.

Realistic Timelines: The recommendation to adjust time estimates by a factor of 5-10x from initial optimistic projections.

Human Oversight (as a concept): The (currently unaffordable) recommendation to have a technical co-founder, advisor, or part-time reviewer to fill validation gaps.

Use a No-Code Platform First: The alternative strategy of proving the business model on a platform like Bubble or Retool before attempting AI-driven development.

